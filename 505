2025-07-22
Python in Machine Learning
Python is the most popular programming language for ML because of its simplicity and rich ecosystem. Its role includes:
Data Handling
Libraries: pandas, numpy, PyTorch
Load, clean, and manipulate datasets
Data Visualization
Libraries: matplotlib, seaborn, plotly, Numpy, Scikit-learn, Ternsoflow, PyTorch, Pandas, 
Explore data patterns and distributions
Model Building
Libraries: scikit-learn (classical ML), tensorflow, pytorch (deep learning)
Train models for regression, classification, clustering, etc.
Evaluation
Metrics: accuracy, precision, recall, RMSE
Test model performance on unseen data
Deployment
Build APIs or integrate models in applications using Flask, FastAPI, or Streamlit.

RAG (Retrieval-Augmented Generation) in ML
RAG is a method combining information retrieval with generative models (like GPT) to improve performance on tasks that require knowledge beyond what a model has seen during training.
How it works:
Retrieval step: Search a database or document corpus for relevant information based on a query.
Generation step: Use the retrieved information to generate answers, summaries, or predictions.
Applications in ML:
Question answering systems
Customer support chatbots
Summarization of documents
Knowledge-based recommendation systems
Integration with Python:
Use Python libraries like transformers (Hugging Face) and faiss (for fast vector search).
Docker is a tool for containerization, which packages an application and its environment into a single, portable container.
Why itâ€™s useful in ML:
Ensures consistent environment: No â€œit works on my machineâ€ issues.
Makes it easier to share and deploy ML models.
Helps manage dependencies: Python version, libraries, CUDA for GPU, etc.
Basic ML workflow with Docker:
Create a Dockerfile specifying the environment, Build the Docker image,  Run the container

machine learning algorithms
Linear Regression, Logistic Regression, KNN(K-Nearest Neighbours), Support Vector, Machines (SVM), Decisions Trees, Naive Bayes
advanced complicated and interesting machine learning
.Supervised Learning(we use labeled data sets to train algorithms to classify data or predict outcomes), using a 
classification model, customer retention have historical data for the customer
regression, we build an equation using various input values with their specific weights determined by the overall value of their impact on the outcome, Airlines rely heavily on various input factors to predict an accurate dollar value for how much they should be charging
.Unsupervised Learning, analyze and cluster unlabeled data sets, discover hidden patterns or groupings without the need for human intervention right so we're using unlabeled data here
clustering,  when organizations try to group similar customers into buckets segmentation, a variety of information like purchase history, their social media activity or website activity, geography 
dimensionality reduction, 
.Semi-Supervised Learning, 
.Reinforcement Learning, an agent or system take actions in an environment and the environment will then either reward the agent for correct moves or punish it for incorrect moves, self-driving cars so autonomous driving has several factors right there's the speed limit there are drivable zones there are collisions and so on







2. Cloud Platforms in Machine Learning
Cloud platforms provide on-demand computing resources and services for ML tasks.
Popular platforms:
AWS (Amazon Web Services): SageMaker for ML training and deployment
Google Cloud Platform (GCP): AI Platform, BigQuery, Vertex AI
Microsoft Azure: Azure ML, Databricks integration
Why use cloud for ML:
Scalability: Train large models on multiple GPUs or TPUs.
Storage: Handle big datasets easily.
Deployment: Serve models as APIs for applications.
Managed services: Pre-built pipelines, automated hyperparameter tuning, model monitoring.
Example ML workflow on cloud:
Upload dataset to cloud storage (S3, GCS, Azure Blob).
Train model on cloud GPU/TPU instance.
Save trained model to cloud storage.
Deploy model as an API endpoint using cloud ML services.

1. What an LLM Is
A type of machine learning model designed to understand, generate, and manipulate natural language.
Trained on massive amounts of text data to learn patterns, grammar, facts, reasoning, and context.
Examples: GPT-4, BERT, LLaMA, Claude, PaLM.
2. Key Features
Text Generation: Can write essays, emails, code, or summaries.
Text Understanding: Can answer questions, classify text, or extract information.
Context Awareness: Remembers context across sentences or even longer conversations.
Few-shot / Zero-shot Learning: Can perform tasks with little or no task-specific training data.
3. How LLMs Work
Input: A prompt or query in natural language.
Processing: The model uses transformer architecture to predict the next word/token based on context.
Output: Generated text, answer, or recommendation.
4. Applications in ML
Chatbots and Virtual Assistants: Customer support, personal assistants.
Content Creation: Writing articles, summaries, code generation.
Information Retrieval: Answering questions from large document collections (RAG).
Translation and NLP Tasks: Sentiment analysis, summarization, language translation.
5. Using LLMs in Python
Libraries: transformers (Hugging Face), openai, langchain.

1. What Are Graph Technologies?
Graph technologies deal with graph data structures, where:
Nodes (vertices) represent entities (e.g., people, products, devices).
Edges (links) represent relationships between entities (e.g., friendship, purchase, communication).
Useful for modeling complex relationships in data that are not easily represented in tables.
2. Key Concepts
Graph Types
Directed / Undirected: Edges have a direction or not.
Weighted / Unweighted: Edges have values representing strength or cost.
Heterogeneous: Nodes/edges can have different types.
Graph Metrics
Degree: Number of edges per node.
Centrality: Importance of a node in the network.
Shortest Path: Minimum distance between nodes.
Graph Representation
Adjacency matrix or adjacency list
Edge list for sparse graphs
3. Graph Technologies / Tools
Graph Databases: Store and query graph data efficiently
Neo4j, Amazon Neptune, TigerGraph
Graph Analytics & ML Libraries:
Python: networkx (graph algorithms), PyTorch Geometric (GNNs), DGL (Deep Graph Library)
4. Applications in Machine Learning
Social Networks: Recommend friends, detect communities.
Knowledge Graphs: Represent facts and relationships (e.g., Google Knowledge Graph).
Fraud Detection: Detect suspicious patterns in transaction networks.
Supply Chain / Logistics: Optimize routes and relationships.
Molecular / Drug Discovery: Analyze chemical compounds as graphs.
5. Graph Neural Networks (GNNs)
Specialized neural networks designed to work on graph-structured data.
Capture node features and relationships to make predictions:
Node classification (e.g., predict user type)
Link prediction (e.g., recommend connections)
Graph classification (e.g., molecule property prediction)
Using PyTorch Geometric

What is Machine Learning?
ARTIFICIAL INTELLIGENCE
A technique which enables machines
to mimic human behaviour
MACHINE 
Subset of Al technique which use statistical methods to enable machines to improve with experience
DEEP LEARNING
Subset of ML which make the computation of multi-layer neural network feasible
Machine learning (ML) is a branch of artificial intelligence (Al) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.

We give Ml:
A dataset (e.g., labeled pictures of fruits) - ML is about learning patterns from data, not hardcoded rules.
A learning algorithm (e.g., decision trees, neural networks) - Algorithms improve with experience (data).
A task (e.g., classify new images) - The quality of learning depends on data, algorithm, and task fit.

Rule-based systems vs Machine Learning systems
Rule-based systems use predefined rules created by humans to make decisions.
A) Rulebase Example
IF
"yes" is equal to uniform_layer_flow
THETA is greater than 45.0
THETA is less than or equal to 90.0
C4 is greater than (Lm/(0.8*(Hs-H0)))
C6 is greater than (Lb/(0.8*(Hs-H0)))
C9 is less than or equal to (Lt/(0.8^(Hs-H0)))
Then
flow_type_ok is confirmed
"V2" is asssigned to flow_type
"No" is assigned to wake_attachment
Find coanda_attachment_value
 
You are tasked with building a simple rule-based email spam filter.
The function rule_based_classifier takes in an email and labels it as 'spam' if it contains any of a few known spam keywords (e.g., "buy now", "free", etc.). Otherwise, it labels the email as 'ham' (non-spam).
def rule_based_classifier(email):
    spam_keywords = ["buy now", "free", "click here", "limited time offer", "urgent"] # Define a list of spam keywords that commonly appear in spam emails    
    email_lower = email.lower() # Convert the email text to lowercase to make keyword matching case-insensitive
    for keyword in spam_keywords: # Loop through each spam keyword
        if keyword in email_lower: # If the keyword appears anywhere in the email, classify it as 'spam'
            return 'spam'
    return 'ham' # If none of the spam keywords are found, classify the email as 'ham' (not spam)

Simple problems:
For which existing solutions require a lot of hand-tuning or long lists of rules
One machine learning algorithm can often simplify code and perform better
Complex problems:
For which there is no good solution at all using a traditional approach
The best machine learning techniques can find a solution
Fluctuating environments:
A machine learning system can adapt to new data
Getting insights:
About complex problems and large amounts of data

| Feature / Aspect          | Rule-based Systems                         | Machine Learning Systems                           |
| ------------------------- | ------------------------------------------ | -------------------------------------------------- |
| Flexibility               | Flexible                                   | Easy to scale                                      |
| Development speed         | Slow parser development                    | Fast development (if datasets available)           |
| Debugging                 | Easy to debug                              | Difficult to debug                                 |
| Training data requirement | Doesn't require a massive training corpus  | Requires training corpus with annotation           |
| Understanding of language | Understanding of the language phenomenon   | No understanding of the language phenomenon        |
| Precision / Recall        | High precision, moderate recall (coverage) | High recall (coverage)                             |
| Learnability              | Requires skilled developers and linguists  | "Learnability" without being explicitly programmed |

     Email Dataset                               #Instance Gathering
           â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                â”‚
Training Set (80%)  Testing Set (20%)            #Training & Testing 
           â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
              Spam / Ham                        # Classification

Automatically learning of patterns
A spam filter based on machine learning techniques automatically learns which words and phrases are good predictors of spam
By detecting unusually frequent patterns of words in the spam examples compared to the ham examples

Application of ML
1. Healthcare and Medical Diagnosis
Disease Detection: ML models are used to identify diseases like cancer, pneumonia and Parkinson's from medical images. They often achieve accuracy comparable to or better than human doctors.
Predictive Analytics: By analyzing patient history and symptoms, models can predict the risk of certain diseases or potential complications.
Drug Discovery: ML accelerates the drug development process by predicting how different compounds will interact, reducing the time and cost of research.
https://www.youtube.com/watch?v=mkiDXTS6-mU
2. Personalized Recommendations and User Experience
Streaming Platforms: Netflix and Spotify suggest shows and songs based on your watching or listening history.
E-commerce: Sites like Amazon recommend products tailored to your preferences, browsing patterns and past purchases.
Social Media: Algorithms curate content feeds, prioritize posts and suggest friends or pages.
https://www.youtube.com/watch?v=3gJmhoLaLIo
3. Smart Assistants and Human-Machine Interaction
Voice Assistants: Tools like Siri, Alexa and Google Assistant convert spoken input into actionable commands.
Voice Search & Transcription: ML enables users to perform hands-free web searches and get transcription during meetings or phone calls.
Chatbots: Businesses use Al-powered chatbots for 24/7 customer support, helping resolve queries faster and more efficiently.
https://youtu.be/7TT4vFWY3BU?si=uK-EDoaoAX4Eduq-
4. Autonomous Vehicles and Smart Mobility
Computer Vision: Recognizing lanes, pedestrians, traffic signals and obstacles.
Sensor Fusion: Combining data from cameras, LiDAR and radar for a 360-degree view.
Behavior Prediction: Anticipating how other drivers or pedestrians may act.
https://www.youtube.com/watch?v=C2rbym6bXM0

Machine Learning pipeline:
Data Acquisition â†’ Data Cleaning â†’ Data Transformation â†’ Feature Engineering â†’ Modeling â†’ Evaluation â†’ Deployment â†’ Monitoring & Updating
                                      â†‘                                                  â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Improving Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Data Acquisition â€“ Collect raw data from databases, sensors, APIs, or files.
Data Cleaning â€“ Handle missing values, remove duplicates, and fix errors.
Data Transformation â€“ Normalize, scale, or encode data into usable formats.
Feature Engineering â€“ Select or create the most relevant features for the model.
Modeling â€“ Train machine learning algorithms on the prepared data.
Evaluation â€“ Test the model on unseen data, measure accuracy/metrics.
If performance is poor â†’ improve model or revisit earlier steps.
Deployment â€“ Put the trained model into production (API, app, system).
Monitoring & Updating â€“ Track performance over time, retrain when data or patterns change.

Generic Machine Learning pipeline: Data Ingestion/Collection
Purpose: Gathering raw data from various sources (databases, APIs, files, streaming platforms, etc.).
Tasks: Identifying relevant data sources, collecting data, and initial data validation (checking for completeness, consistency).
There are various strategies for gathering relevant data:
Use public datasets
Use public APIs
Data augmentation
https://datasetsearch.research.google.com/
(kaggle, Google, Dataset Search Beta, VisualData, UC Irvine, Machine Learning Repository, OpenML, DATA HUB, DATA, HealthData.gov, Hugging Face, MNIST Dataset, IMDB dataset)

Using public API
| API           | Description                                                                                             |
| ------------- | ------------------------------------------------------------------------------------------------------- |
| IPstack       | Locate and identify website visitors by IP address                                                      |
| Marketstack   | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format              |
| Weatherstack  | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format |
| Numverify     | Global phone number validation & lookup JSON API                                                        |
| Fixer         | Simple and lightweight API for current and historical foreign exchange (forex) rates                    |
| Aviationstack | Free, real-time flight status and global aviation data API                                              |
| API                | Use Case                                   |
| ------------------ | ------------------------------------------ |
| Reddit API         | Opinion mining, conversational modeling    |
| News API           | Topic modeling, news summarization         |
| YouTube Data API   | Emotion analysis, speech-to-text data      |
| Wikipedia API      | Pretraining language models, summarization |
| Stack Exchange API | Intent classification, question answering  |

Data Augmentation
Data augmentation is the process of increasing the diversity and size of training datasets without actually collecting new data.
This is especially valuable when datasets are small or imbalanced, and it helps improve model generalization, reduce overfitting, and boost performance.

Original Image â†’ Horizontal â†’ 
                  Vertically â†’ 
                  +45 Rotation â†’ 
                  -45 Rotation â†’             
                  Blur â†’                             â”‚  
                  Brighter â†’                         â””â”€> "This is very cool" IMAGE -> / "This is pretty cool" / "This is really cool" / "This is super cool" / "This is kinda cool"
                  Noise Added â†’ 
                  Darker â†’ 
                  Grayscale â†’ 
                  Crop â†’ 
                  Augmented Images

Generic Machine Learning pipeline: Data Cleaning
Purpose: Transforming raw data into a clean, structured, and suitable format for ML analysis. 
Tasks:
Handling Missing Values
Handling Outliers
Data Cleaning
Handling Missing Values:
Missing values are data points that are absent for a specific variable in a dataset.
Handling Outliers
An outlier is also a data point that is drastically different from the other records in the dataset, with the differences being either too high or too low when compared to the rest of the observations.
Data Cleaning
@YMourri and @AndrewYNg are tuning a GREAT Al model at https://deeplearning.ai!!!
Preprocessed tweet:[tun, great, ai, model]

      Feature rngineering
Feature extraction/engineering is an important step for any machine learning problem
No matter how good a modeling algorithm you use, if you feed in poor features, you will get poor results
In computer science, this is often called "garbage in, garbage out"
Feature representation is a common step in any ML project
Whether the data is text, images, videos, or speech
A digital black-white image image contains discrete number of pixels.
The image is stored in a computer in the form of a matrix of pixels.
The real value stored at cell[i,j] represents the intensity of the corresponding pixel in the image
Pixel value: shows an intensity
pixel(x,y) value"grayscale" 75
A digital color image image contains discrete number of pixels
-Pixel value: shows an intensity value
"grayscale"
"color"
f(x, y) =r(x, y)g(x, y)b(x, y)
[90, 0, 53][249, 215, 203][213, 60, 67]
A video is just a collection of frames where each frame is an image
Hence, any video can be represented as a sequential collection of matrices, one per frame, in the same order.
But it turns out that representing text is not straightforward
In contrast to images, video, and speech that can be mathematically represented in a straightforward manner
The conversion of raw text to a suitable numerical form is called text representation
Text representation has been an active area of research in the past decades.
Feature: Word | Word embedding | Dimensionality reduction | Visualization of word embeddings in 2D

Types of Machine Learning Algorithms
| **Category**                   | **Types**                                                                                    |
| ------------------------------ | -------------------------------------------------------------------------------------------- |
| **Based on the Task**          | Supervised learning, Unsupervised learning, Semi-supervised learning, Reinforcement learning |
| **Based on Updating Approach** | Batch learning, Online learning                                                              |
| **Based on Generalization**    | Instance-based learning, Model-based learning                                                |
| **Based on Data Assumptions**  | Parametric algorithms, Nonparametric algorithms                                              |

Supervised learning
- A **dataset** is a collection of labeled examples:  
  \{(x_i, y_i)\}_{i=1}^N
- Each element \(x_i\) among \(N\) is called a **feature vector**.  
- A **feature vector** is a vector in which each dimension \(j = 1, \dots, D\) contains a value that describes the example.  
- That value is called a **feature** and is denoted as \(x_i^{(j)}\).  
- The **label** \(y_i\) can be:  
  - an element belonging to a finite set of classes \(\{1, 2, \dots, C\}\),  
  - a real number, or a graf
The goal is to use the dataset to produce a model
The model takes a feature vector x as input
It outputs information that allows deducing the label y for this feature vector

In machine learning domain we have several types of mathematical objects:
Scalar Vector Matrix Tensor

Classification vs Regression
Regression
What will be the temperature tomorrow?
Classification
Will it be hot or cold tomorrow?
Ð ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ (Regression):
ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ.
ÐÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ â€” ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð»ÑŽÐ±Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ð² Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ð¾Ð¼ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ðµ, Ð° Ð½Ðµ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹.
ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°: 23.5Â°C, 24.1Â°C, 23.99Â°C
Ð¦ÐµÐ½Ð° Ð´Ð¾Ð¼Ð°: 150000, 150250.5, 150123.75
Ð’ÐµÑ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°: 70.2 ÐºÐ³, 70.25 ÐºÐ³
ÐŸÑ€Ð¸Ð¼ÐµÑ€: Â«ÐšÐ°ÐºÐ°Ñ Ð±ÑƒÐ´ÐµÑ‚ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð·Ð°Ð²Ñ‚Ñ€Ð°?Â»
Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð»ÑŽÐ±Ñ‹Ð¼ Ñ‡Ð¸ÑÐ»Ð¾Ð¼ Ð² Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ 23.5Â°C.
ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ (Classification):
ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ Ð¸Ð»Ð¸ ÐºÐ»Ð°ÑÑ.
ÐŸÑ€Ð¸Ð¼ÐµÑ€: Â«Ð‘ÑƒÐ´ÐµÑ‚ Ð»Ð¸ Ð·Ð°Ð²Ñ‚Ñ€Ð° Ð¶Ð°Ñ€ÐºÐ¾ Ð¸Ð»Ð¸ Ñ…Ð¾Ð»Ð¾Ð´Ð½Ð¾?Â»
Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚ Ðº ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾Ð¼Ñƒ Ð½Ð°Ð±Ð¾Ñ€Ñƒ ÐºÐ»Ð°ÑÑÐ¾Ð², Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Â«Ð¶Ð°Ñ€ÐºÐ¾Â» Ð¸Ð»Ð¸ Â«Ñ…Ð¾Ð»Ð¾Ð´Ð½Ð¾Â».
Quantitative and Qualitative variables
Quantitative variables take on numerical values
Examples include a person's age or income
Qualitative variables take on values in one of C different classes, or categories
E.g., a cancer diagnosis (yes or no)
Regression
Refers to problems with a quantitative target label
Classification
Refers to problems with a qualitative target label
Classification
Goal: Predict a category or class label.
Output: Discrete values (e.g., "yes"/"no", "spam"/"not spam", digits 0-9).
Examples:
Email spam detection (spam or not)
Disease diagnosis (positive or negative)
Image recognition (cat, dog, car, etc.)

Regression
Goal: Predict a numerical value.
Output: Continuous values (real numbers).
Examples:
Predicting house prices
Forecasting temperature

Regression: Linear Regression, Poisson Regression, Support Vector Regression 
Classification: Logistic Regression, Decision Tree, NaÃ¯ve Bayes Classifier, Neural Network

Unsupervised Learning
The dataset is a collection of unlabeled examples {x}-1
Again, x is a feature vector
The goal is to create a model
The model takes a feature vector x as input
It either transforms the feature vector into another vector or into a value that can be used to solve a practical problem

| Ð¢Ð¸Ð¿ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°                                         | ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹ / ÐœÐµÑ‚Ð¾Ð´Ñ‹                                      |
| ----------------------------------------------------- | ------------------------------------------------------- |
| **Clustering (ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ)**                        | K-Means, Hierarchical, Fuzzy C-Means                    |
| **Dimensionality Reduction (Ð¡Ð½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸)**   | Principal Component Analysis (PCA), Kernel PCA          |
| **Association (ÐÑÑÐ¾Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° / Data Mining)** | Apriori Algorithm, Eclat Algorithm, FP-Growth Algorithm |

Clustering
Clustering is the task of grouping a set of objects
In such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).
For example, a set of customers, a clustering algorithm can group them basec on their similarity

Dimensionality Reduction
The goal is to simplify the data without losing too much information. One way to do this is to merge several correlated features into one.

Anomaly Detection
The goal is the identification of rare items, events or observations.
Which raise suspicions by differing significantly from the majority of the data.
For example
Detecting unusual credit card transactions to prevent fraud.
Catching manufacturing defects.
Automatically removing outliers from a dataset before feeding it to another learning algorithm.

Semi-Supervised Learning
The dataset contains both labeled and unlabeled examples
Usually, the quantity of unlabeled examples is much higher.
The goal is the same as the goal of the supervised learning algorithm.
Most semi-supervised learning algorithms are combinations of unsupervised and supervised algorithms
| Concept / Method            | Description / Example                                                                         |
| --------------------------- | --------------------------------------------------------------------------------------------- |
| **Embedding method**        | ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð² Ð¸Ð»Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ð²Ð¸Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² (*word embeddings*)                           |
| **Machine learning model**  | Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ ML                                               |
| **Learning task**           | Self-supervised: Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð±ÐµÐ· ÑÐ²Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ð¾Ðº (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Â«I think \[???] I amÂ») |
| **Relation to supervision** | Self-supervised â‰ˆ unsupervised + supervised elements                                          |
| **Meaning**                 | Ð—Ð°ÐºÐ»Ð°Ð´Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ¼Ñ‹ÑÐ» Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ÑÐ»Ð¾Ð² Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾                                    |

Reinforcement Learning (akaGPT)
Reinforcement learning is a subfield of machine learning where the machine "lives" in an environment
The machine is capable of perceiving the state of that environment as a vector of features
The machine can execute actions in every state
Different actions bring different rewards
Actions could also move the machine to another state of the environment
The goal is to learn a policy
A policy is a function (similar to the model in supervised learning)
The policy takes the feature vector of a state as input
It outputs an optimal action to execute in that state
The action is optimal if it maximizes the expected average reward.




2025-07-29
https://www.youtube.com/watch?v=SUbqykXVx0A
The machine is capable of perceiving the state of that environment as a vector of features
The machine can execute actions in every state
Different actions bring different rewards
Actions could also move the machine to another state of the environment

Reinforcement learning is a subfield of machine learning where the machine
"lives" in an environment
Semi-Supervised Learning
Embedding method
Machine learning model
Learning task
"I think [???] I am"
Self-supervised = unsupervised + supervised
Meaning
Word embeddings
Tasks (e.g., Classification, Regression)
The dataset contains both labeled and unlabeled examples
Usually, the quantity of unlabeled examples is much higher.
The goal is the same as the goal of the supervised learning algorithm.
Most semi-supervised learning algorithms are combinations of unsupervised and supervised algorithms
For example
Detecting unusual credit card transactions to prevent fraud.
Catching manufacturing defects.
Automatically removing outliers from a dataset before feeding it to another learning algorithm.
The goal is the identification of rare items, events or observations.
Which raise suspicions by differing significantly from the majority of the data.
The goal is to simplify the data without losing too much information.
One way to do this is to merge several correlated features into one.
Clustering is the task of grouping a set of object
In such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).
Training set
Feature 2
For example, a set of customers, a clustering algorithm can group them basec on their similarity

Unsupervised Learning
Clustering	
- K-Means
- Hierarchical
- Fuzzy C-Means
- Polynomial
Dimensionality Reduction	
- Principal Component Analysis (PCA)
- Kernel Principal Analysis
Association (Data Mining)	
- Apriori Algorithm
- Eclat Algorithm
- FP-Growth Algorithm

N The dataset is a collection of unlabeled examples {x}=1
Again, x is a feature vector
The goal is to create a model
The model takes a feature vector x as input
It either transforms the feature vector into another vector or into a value that can be used to solve a practical problem

Regression:
Linear Regression
Poisson Regression
Logistic Regression
Support Vector Regression
Neural Network

Classification:
Decision Tree
NaÃ¯ve Bayes Classifier
Neural Network
Support Vector Regression (ÐµÑÐ»Ð¸ ÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸ÑŽ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¹)

Classification:
Goal: Predict a category or class label
Output: Discrete values (e.g., "yes"/"no", "spam"/"not spam", digits 0â€“9)
Examples:
Email spam detection (spam or not)
Disease diagnosis (positive or negative)
Image recognition (cat, dog, car, etc.)

Regression:
Goal: Predict a numerical value
Output: Continuous values (real numbers)
Examples:
Predicting house prices
Forecasting temperature
Value

Quantitative and Qualitative variables
Quantitative variables take on numerical values
Examples include a person's age or income
Qualitative variables take on values in one of C different classes, or categories
E.g., a cancer diagnosis (yes or no
Regression - Refers to problems with a quantitative target label
Classification - Refers to problems with a qualitative target label

Supervised Learning: Classification - Regression

Dataset notation:

ð‘={(ð‘¥ð‘–,ð‘¦ð‘–)}=ð‘ð‘–=1â€” our dataset (table)
ð‘=4 is the total number of data points (rows)
D=3 â€” total number of features (columns, excluding the target label column)

ð‘‹1 - is the first feature vector
ð‘¥3  â€” third feature of the second feature vector 
ð‘¦3 â€” target label of the third feature vector

Example features (for a housing dataset):
Rooms|Area|District|Price (target label)

Supervised Learning
The goal is to use the dataset to produce a model
The model takes a feature vector x as input
It outputs information that allows deducing the label y for this feature vector
The dataset is the collection of labeled examples {(xi, Yi)}=1
Each element xâ‚ among N is called a feature vectorA feature vector is a vector in which each dimension j = 1, ..., D contains a value that describes the example somehow
That value is called a feature and is denoted as x) (x superscript j)
The label yâ‚ can be either an element belonging to a finite set of classes {1, 2,..., C}, or a real number, or a more complex structure
Like a vector, a matrix, a tree, or a graph

Types of Machine Learning Algorithms
1. Based on the task:
Supervised learning
Unsupervised learning
Semi-supervised learning
Reinforcement learning
2. Based on the updating approach:
Batch learning
Online learning
3. Based on generalization approach:
Instance-based learning
Model-based learning
4. Based on the assumptions about data:
Parametric algorithms
Nonparametric algorithms

Machine Learning pipeline:
Data Acquisition â†’ Data Cleaning â†’ Data Transformation â†’ Feature Engineering â†’ Modeling â†’ Evaluation(â†’ Improving the Model->Data trans) â†’ Deployment â†’ Monitoring and Model Updating 

Batch Learning:
Train â†’ Train  â†’ Modelâ¬‡  -----â¬‡
      In time   Validation    â¬‡
Test ---------------â†’     Scoring

Online Learning:
Assumed Model â†’ Covariates of Observation 1 â†’  Prediction â†’ (+ Actual Response) â†’ Evolved Model â†’ (Covariates of Observation 2) â†’ Prediction

| **Perspective**                | **Batch Learning**                            | **Online Learning**                                               |
| ------------------------------ | --------------------------------------------- | ----------------------------------------------------------------- |
| **When is data available?**    | All data is available up front                | Data arrives sequentially over time                               |
| **How is the model trained?**  | Trained once on a full dataset                | Updated continuously with each new data point                     |
| **When is the model updated?** | Only when retrained (offline)                 | After every observation (real-time)                               |
| **Ideal use case**             | Static environments (e.g., medical diagnosis) | Dynamic environments (e.g., stock prices, recommendation systems) |

Batch Learning
Batch learning involves training a model on a the entire dataset, and then used to make predictions on new data.
Batch learning is commonly used in situations where the dataset is relatively small and can be processed quickly.
If you want a batch learning system to know about new data (such as a new type of spam), you need to:
First, train a new version of the system from scratch on the full dataset (Not just the new data, but also the old data)->
Then, stop the old system and replace it with the new one

Online learning
Online learning, also known as incremental learning or streaming learning, involves training a model on new data as it arrives, one observation at a time.
The model is updated each time a new observation is received, allowing it to adapt to changes in the data over time.
Online learning is commonly used in situations where the data is too large to be processed all at once, or where the data is constantly changing, such as in stock market data or social media data.

Instance-Based vs Model-Based Learning
| **Perspective**                   | **Instance-Based Learning**                      | **Model-Based Learning**                           |
| --------------------------------- | ------------------------------------------------ | -------------------------------------------------- |
| **How is knowledge stored?**      | Keeps the training data (instances)              | Learns a general model from the data               |
| **How does it make predictions?** | By comparing new input to stored examples        | By applying a learned function or parameters       |
| **When does learning happen?**    | Mostly at prediction time (lazy learning)        | During training phase (eager learning)             |
| **Examples**                      | k-Nearest Neighbors (k-NN), Case-Based Reasoning | Linear Regression, Decision Trees, Neural Networks |

Idea: "Remember and compare."
It keeps all the examples and looks at the most similar ones when making a prediction.
Example: k-Nearest Neighbors (k-NN)
vs
Idea: "Learn a pattern."
It creates a model from the training data and uses it to predict new results.
Examples: Linear Regression, Neural Networks

Parametric and Nonparametric Algorithms
What Parametric Algorithms do: Assume the data follows a known shape (like a straight line).
They learn: A fixed number of parameters (e.g., slope & intercept in a line).
Fast to train, but less flexible.
Examples:
Linear Regression
Logistic Regression
Naive Bayes
vs
What Nonparametric Algorithms do: Don't assume a fixed shape or number of parameters.
They learn patterns freely from the data, even if it's complex.
More flexible, but need more data.
Examples:
k-Nearest Neighbors (k-NN)
Decision Trees
Random Forests
Support Vector Machines (with kernels)

| **Aspect** | **Hyperparameters**                                     | **Parameters**       | **Score** |
| ---------- | ------------------------------------------------------- | -------------------- | --------- |
| Example 1  | n\_layers = 3, n\_neurons = 512, learning\_rate = 0.1   | Weights optimization | 85%       |
| Example 2  | n\_layers = 3, n\_neurons = 1024, learning\_rate = 0.01 | Weights optimization | 80%       |
| Example 3  | n\_layers = 5, n\_neurons = 256, learning\_rate = 0.1   | Weights optimization | 92%       |

ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (parameters) â€” ÑÑ‚Ð¾ Ð²ÐµÑÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ð½Ð° ÑÐ°Ð¼Ð° Ð¿Ð¾Ð´Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.
Ð“Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (hyperparameters) â€” ÑÑ‚Ð¾ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‘Ñ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð´Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ñ‘Ð², ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ).
Parameters
Are variables that define the model
Are directly modified by the learning algorithm based on the training data
The goal of learning is to find such values of parameters that make the model optimal in a certain sense
Hyperparameters
Are properties of a learning algorithm, usually having a numerical value
Influence the way the algorithm works
Are not learned by the algorithm itself from data
They have to be set by the data analyst before running the algorithm.

Evalustion - Measuring the Quality of Fit
We need some way to measure how well its predictions actually match the observed data
To evaluate the performance of a statistical learning method on a given data set
The most commonly-used measure
For regression is the mean squared error (MSE)
For classification is the accuracy

| **Metric**                   | **Use For**    | **Goal**                     | **Formula Meaning**                              | **Formula**                                                    |
| ---------------------------- | -------------- | ---------------------------- | ------------------------------------------------ | -------------------------------------------------------------- |
| **MSE (Mean Squared Error)** | Regression     | Minimize error               | Penalizes squared differences                    | $\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2$     |
| **Accuracy**                 | Classification | Maximize correct predictions | Measures match between true and predicted labels | $\text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} 1(y_i = f(x_i))$ |
MSE (Ð¡Ñ€ÐµÐ´Ð½ÐµÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°)
Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸ (ÐºÐ¾Ð³Ð´Ð° Ð¼Ñ‹ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾).
Ð¡Ð¼Ñ‹ÑÐ»: Ñ‡ÐµÐ¼ Ð¼ÐµÐ½ÑŒÑˆÐµ MSE, Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.
ÐžÐ½Ð° ÑÐ¸Ð»ÑŒÐ½Ð¾ ÑˆÑ‚Ñ€Ð°Ñ„ÑƒÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸, Ñ‚Ð¾ ÐµÑÑ‚ÑŒ ÐµÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€Ð¾Ð¼Ð°Ñ…Ð½ÑƒÐ»Ð°ÑÑŒ Ð´Ð°Ð»ÐµÐºÐ¾, ÑÑ‚Ð¾ Ð·Ð°Ð¼ÐµÑ‚Ð½Ð¾ ÑƒÑ…ÑƒÐ´ÑˆÐ°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚.
Accuracy (Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ)
Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ (ÐºÐ¾Ð³Ð´Ð° Ð¼Ñ‹ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑ, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Â«Ð´Ð°/Ð½ÐµÑ‚Â»).
Ð¡Ð¼Ñ‹ÑÐ»: Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, ÐºÐ°ÐºÐ°Ñ Ð´Ð¾Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ ÑÐ¾Ð²Ð¿Ð°Ð»Ð° Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ð¼Ð¸.
Ð§ÐµÐ¼ Ð±Ð»Ð¸Ð¶Ðµ Ðº 100%, Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.

Training Set vs Test Set
Training set | Validation Test Set | Test Set
Train on training set, tune on validation, report on testset
This avoids overfitting ('tuning to the test set')
More conservative estimate of performance

Underfitting
Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples and the target values.
Overfitting
Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.
Add More Training Data â€“ More data helps the model generalize better.
Use Regularization (L1/L2, Dropout)
L1/L2 Regularization (Ridge/Lasso) adds a penalty to large weights.
Dropout (for neural networks) randomly deactivates neurons during training.
Early Stopping - Stop training when validation loss stops improving.
Reduce Model Complexity - Decrease the number of layers, neurons, or use a simpler algorithm.
Use Data Augmentation - Apply transformations like rotation, flipping, cropping (for images), or synonyms and paraphrasing (for text).
Cross-Validation - Use techniques like k-fold cross-validation to ensure the model generalizes well.
Hyperparameter Tuning â€“ Adjust parameters like learning rate, batch size, number of trees (for decision trees), etc.
But paradox: want as much data as possible for training, and as much for validation; how to split?

Cross-validation:
K-folds (K-Fold Cross-validation In cross-validation, we choose a number k, and partition our data into k disjoint subsets called folds.)
Bootstrapping
Hold-out
Leave-one-out
Leave-p-out
Stratified K-folds
Repeated K-folds
Nested K-folds
Complete

Training error
-The MSE/accuracy can be computed using the training data that was used to fit the model
-It is referred to as the training MSE (or accuracy)
Test error
-Rather, we are interested in the correctness of the predictions that we obtain when we apply our method to previously unseen test data
-It is referred to as the test MSE/accuracy
We want to choose the method that gives the lowest test MSE (or highest test accuracy)

ÑÑ‚Ð°Ð¿Ñ‹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Data Science / Machine Learning:
| **Stage**                          | **Tasks / Actions**                                                                                                                                                             |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Business Problem Understanding** | Defining the problem, Formulating it as a DS/ML task                                                                                                                            |
| **Data Collection**                | Identifying data sources, Integrating datasets, Splitting the data into training and test sets                                                                                  |
| **Data Exploration**               | Deciding on evaluation metrics, Deciding on data cleaning strategies, Deciding on resampling, Deciding on encoding categorical attributes, Deciding on dimensionality reduction |
| **Data Preprocessing**             | Data cleaning, Data resampling                                                                                                                                                  |
| **Feature Engineering**            | Feature extraction, Feature selection, Feature transformation, Dimensionality reduction                                                                                         |
| **Model Training**                 | Algorithm selection, Hyperparameter tuning, Model validation                                                                                                                    |
| **Model Evaluation**               | Test set preparation, Applying the trained model, Computing the evaluation metrics                                                                                              |
| **Post-Modeling Steps**            | Model deployment, Model monitoring and updating, Model explanation, Online evaluation with A/B testing, Building dashboards, Data-driven decision making                        |


Scikit-Learn Key Features Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ð´Ð»Ñ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Python
Consistent API Design: Provides a uniform interface across different machine learning algorithms, making it easy to switch models with minimal code changes.
Built-in Datasets: Includes several small, standard datasets like Iris and Digits for testing and experimentation.
Preprocessing Tools: Offers functions for scaling, normalizing, encoding categorical variables, imputing missing values, and more.
Wide Range of Algorithms: Supports various models for classification, regression, clustering, and dimensionality reduction.
Model Evaluation Metrics: Includes functions to calculate accuracy, precision, recall, F1-score, ROC AUC, and other metrics to assess model performance.

How do you turn a real-world business problem into a machine learning task?
Why should data be split into training and test sets at the very beginning of a project?
What is the difference between i.i.d. datasets and time series datasets, and why does it matter for data splitting?
What are common issues in raw data (e.g., missing values, outliers, imbalanced classes) that must be addressed before modeling?

You turn a real-world business problem into a machine learning task by translating the goal into a clear prediction or decision that an algorithm can learn from data.
Steps:
Understand the goal: Identify what the business wants to achieve (We need to predict whether a particular customer will stay with us).
Define the prediction target: Decide what you are trying to predict (e.g., churn = Yes/No).
Business Problem Understanding > Data Collection > Data Exploration > Data Preprocessing > Feature Engineering > Model Training > Model Evaluation

You turn a real-world business problem into a machine learning task by translating the goal into a clear prediction or decision that an algorithm can learn from data.
Steps:
Choose the task type: Frame it as a classification, regression, or other ML task based on the target:
Classification â†’ predicting categories (e.g., churn or not).
Regression â†’ predicting a continuous value (e.g., sales amount).
Identify available data â€“ Gather data that links inputs (features) to the target.

Example:
If a product manager says, â€œWe need to know if a customer will stay with us," you can frame this as a classification task where the model predicts whether a customer is likely to stay (1) or leave (0).
A set of business problems phrased as real-world scenarios.
Exercise: Business Problem Understanding
You can be asked to decide how to formulate them into machine learning tasks:
A real estate company wants to estimate the price of a house based on features like location, size, and age.
Answer: This is a regression problem because the target variable (price) is a continuous number.
A manufacturing company wants to detect faulty products on the assembly line using images.
Answer: This is a classification problem because the model must assign each product to a category: "faulty" or "not faulty."
"A hospital wants to group patients with similar symptoms to discover unknown disease subtypes.
Answer: This is a clustering problem because the goal is to group patients based on similarity in their symptoms without predefined labels, in order to discover unknown disease subtypes.
Answer: This is a clustering problem (unsupervised learning) since there are no predefined labels; the goal is to discover natural groupings in the data.

Data Collection
We then need to collect relevant data for the task
E.g., "here are the logs of customers' interactions with our product for five years"
It is better to right away split the data into train and test
To not learn anything from the test set

i.i.d. Datasets vs Time Series Datasets
In i.i.d. (independent and identically distributed) datasets, the data points are independent:
Allowing for random splits between training and test sets.
In time series datasets, the data points have a temporal order:
So, splitting randomly would break the temporal structure and introduce data leakage.
Instead, the data should be split chronologically, ensuring the model learns from past data to predict future outcomes.
| Feature 1 | Feature 2 | Label |
| --------- | --------- | ----- |
| 5         | 2.3       | Yes   |
| 3         | 4.1       | No    |
| 7         | 2.1       | Yes   |

| Time       | Feature 2 | Label |
| ---------- | --------- | ----- |
| 01.12.2000 | 2.3       | Yes   |
| 02.12.2000 | 4.1       | No    |
| 03.12.2000 | 2.1       | Yes   |

1. Finance
Example: Stock Prices
Dataset: Daily closing prices of Apple (AAPL) from 2010 to 2025.
Fields: Date, Open, High, Low, Close, Volume
Application: Forecasting future prices, volatility analysis, algorithmic trading
2. Healthcare
Example: Patient Heart Rate Monitoring
Dataset: Heart rate recorded every minute from wearable devices.
Fields: Timestamp, Heart Rate (BPM), Patient ID
Application: Detecting arrhythmia, stress monitoring, early warning systems

Business Problem:
A real estate company wants to estimate the price of a house based on features like location, size, and age.
Question: Find publicly available datasets you can use for the housing price estimation business problem
Exploratory Data Analysis
House Price Prediction Dataset



















